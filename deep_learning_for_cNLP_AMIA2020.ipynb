{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_learning_for_cNLP_AMIA2020.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/turbosheep/deep_learning_for_cNLP_AMIA2020/blob/master/deep_learning_for_cNLP_AMIA2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eL_3Sbt5m9b6",
        "colab_type": "text"
      },
      "source": [
        "# Python Deep Learning for Clinical NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFtd10hknSbT",
        "colab_type": "text"
      },
      "source": [
        "### Presented by Olga Patterson and Hannah Eyre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLOAWiwMV-cp",
        "colab_type": "text"
      },
      "source": [
        "## Disclaimer\n",
        "The contents of this presentation do not represent the views of the Department of Veterans Affairs or the United States Government."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPffAPhDV6cG",
        "colab_type": "text"
      },
      "source": [
        "## Acknowledgement\n",
        "Some work described in this presentation was supported using resources and facilities at the VA Salt Lake City Health Care System with funding from VA Informatics and Computing Infrastructure (VINCI), VA HSR RES 13-457"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf-Gaat8nkuv",
        "colab_type": "text"
      },
      "source": [
        "## About this Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JeBDu5rWK8F",
        "colab_type": "text"
      },
      "source": [
        "This colab notebook is available at: https://tinyurl.com/cNLP-AMIA2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p25SEXbCEpN",
        "colab_type": "text"
      },
      "source": [
        "### Goals\n",
        "\n",
        "\n",
        "1.   Understand when python might be the right choice for deep learning on clinical text.\n",
        "2.   Understand how deep learning for NLP is different from other deep learning problems.\n",
        "3.   Understand the steps to set up a deep learning model for an NLP task.\n",
        "4.   Understand how to examine results of a deep learning system.\n",
        "5.   Understand how to use and improve a deep learning system.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kip0l-3CDVMO",
        "colab_type": "text"
      },
      "source": [
        "### Task\n",
        "\n",
        "The goal of this tutorial is to identify three common entities in text:\n",
        "\n",
        "*   Diseases/Syndromes (COVID-19, respiratory failure, etc.)\n",
        "*   Anatomical Site (lung, lower extremity, etc.)\n",
        "*   Pharmacologic Substances (vaccine, gene names, proteins, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW8HQrH1DXVR",
        "colab_type": "text"
      },
      "source": [
        "### Data Source\n",
        "\n",
        "The data used in this demo is from the [COVID-19 Open Research Dataset (CORD) ](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge). This dataset contains over 200,000 scholarly articles related to COVID-19 and other coronaviruses. This is *NOT* clinical text, but it shares a similar scientific vocabulary, does not contain PHI, and is freely available to download.\n",
        "\n",
        "For this demo, only the abstracts of the articles are used. Additionally, the data has been annotated algorithmically. This means there is no quality assurance for the data and should only be used for learning and demonstration purposes. The methods shown here can apply to any data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jz-ZjNFB9Hc",
        "colab_type": "text"
      },
      "source": [
        "### Computing Environment\n",
        "This demo will run in the cloud using Google Colab, which requires no setup by participants. It is compatible with tablets and smartphones. More information about the computing environment can be found at the [Colab FAQ](https://research.google.com/colaboratory/faq.html).\n",
        "\n",
        "This code requires an internet connection to run, but will not download or run anything onto your computer.\n",
        "\n",
        "***NOTE:*** This environment is not secure and data containing PHI should not be uploaded or used in a Colab environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHonZjmnoF0A",
        "colab_type": "text"
      },
      "source": [
        "# Why Python?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8rD8daOTNS5",
        "colab_type": "text"
      },
      "source": [
        "When SHOULD you choose python for your deep learning project?\n",
        "\n",
        "1.   Robust open source community. You take advantage of the benefits the work of many experienced developers and scientists, not just what you can program by yourself.\n",
        "2.   There are a variety of packages to choose from when starting a project depending on your preferences and needs.\n",
        "3.   Lots of help. Major packages have millions of active users. Experienced users and developers are always available to help troubleshoot via email, social media, slack, and stackoverflow.\n",
        "4.   You need to program something from scratch.\n",
        "\n",
        "\n",
        "When SHOULDN'T you choose python for your deep learning project?\n",
        "\n",
        "1.   Your project needs robust package management. While Anaconda is advancing python package management, it does not have as many features as Maven for Java.\n",
        "2.   You need to prioritize energy-efficiency or speed. C or C++ will give the fastest, most energy efficient systems. Also consider non-deep learning solutions for this.\n",
        "3.   You do not have experience programming. Python is a beginner friendly language, however there are tools available that allow you to train models without any programming required.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxd9E6GtoOAj",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqYph3GjCaFk",
        "colab_type": "text"
      },
      "source": [
        "Deep learning has a small amount of randomness to each training iteration. While this should not affect the results of a properly built system, this tutorial will keep everything consistent by freezing the random number generators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVB4vuZCCa4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(123)\n",
        "\n",
        "import random\n",
        "random.seed(123)\n",
        "\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF_vFD76opW9",
        "colab_type": "text"
      },
      "source": [
        "## Getting the Data\n",
        "\n",
        "This data is being downloaded from a public github repository set up for the demo [here](https://github.com/turbosheep/deep_learning_for_cNLP_AMIA2020).\n",
        "\n",
        "This demo will only use a fraction of the abstracts available, but the full dataset is available there. The preprocessing scripts that split the data into this downloaded format are also available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhWrJvB6A0-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/turbosheep/deep_learning_for_cNLP_AMIA2020/master/test_train_split/test_data_pretokenized.tsv\n",
        "!wget https://raw.githubusercontent.com/turbosheep/deep_learning_for_cNLP_AMIA2020/master/test_train_split/test_labels_bio.tsv\n",
        "\n",
        "!wget https://raw.githubusercontent.com/turbosheep/deep_learning_for_cNLP_AMIA2020/master/test_train_split/train_data_pretokenized.tsv\n",
        "!wget https://raw.githubusercontent.com/turbosheep/deep_learning_for_cNLP_AMIA2020/master/test_train_split/train_labels_bio.tsv\n",
        "\n",
        "!wget https://raw.githubusercontent.com/turbosheep/deep_learning_for_cNLP_AMIA2020/master/test_train_split/validation_data_pretokenized.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CiKiRW2Bfd5",
        "colab_type": "text"
      },
      "source": [
        "## Opening data\n",
        "\n",
        "The data has been broken into a training set, a test set, and a validation set. The training and test set come with annotations on the data, but the validation does not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hafT4TLpBfDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_raw = []\n",
        "with open(\"train_data_pretokenized.tsv\") as f:\n",
        "  for line in f.readlines():\n",
        "    train_data_raw.append(line.strip().split('\\t'))\n",
        "\n",
        "train_labels_raw = []\n",
        "with open(\"train_labels_bio.tsv\") as f:\n",
        "  for line in f.readlines():\n",
        "    train_labels_raw.append(line.strip().split('\\t'))\n",
        "\n",
        "test_data_raw = []\n",
        "with open(\"test_data_pretokenized.tsv\") as f:\n",
        "  for line in f.readlines():\n",
        "    test_data_raw.append(line.strip().split('\\t'))\n",
        "\n",
        "test_labels_raw = []\n",
        "with open(\"test_labels_bio.tsv\") as f:\n",
        "  for line in f.readlines():\n",
        "    test_labels_raw.append(line.strip().split('\\t'))\n",
        "\n",
        "validation_data = []\n",
        "with open(\"validation_data_pretokenized.tsv\") as f:\n",
        "  for line in f.readlines():\n",
        "    validation_data.append(line.strip().split('\\t'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBCa3gx4G3qR",
        "colab_type": "text"
      },
      "source": [
        "## Data Format\n",
        "\n",
        "If we examine our training data, we can see that there is a long hexadecimal ID followed by some text. This ID is used by the original CORD data, but we can also use it to pair our data to our labels.\n",
        "\n",
        "Each row in the data represents a sentence with at least one annotation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoGXgbbvCmIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_raw[7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9VZuTgWecXb",
        "colab_type": "text"
      },
      "source": [
        "Our labels are in the BILUO format, a variant of the common BIO format.\n",
        "\n",
        "*   ***B***egin: the first token of a 2 or more token sequence\n",
        "*   ***I***nside: a token inside a 3 or more token sequence\n",
        "*   ***L***ast: the last token of a 2 or more token sequence\n",
        "*   ***U***nit: a one token length sequence\n",
        "*   ***O***utside: a token not in any labeled sequence\n",
        "\n",
        "Each label will be one of the BILUO schema followed by the specific type: B-DISEASE, L-PHARM, etc.\n",
        "\n",
        "Each row will match with the same row in the training data. Each BILUO label will correspond in-order with the token in the text.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JEvfAYOExda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels_raw[7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "315kBz9OmexI",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUqRanMExKzc",
        "colab_type": "text"
      },
      "source": [
        "## Challenges\n",
        "\n",
        "Deep learning for NLP tasks have a variety of (often unspoken) challenges that need to be handled in the model design and preprocessing phase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urPzWwMixdls",
        "colab_type": "text"
      },
      "source": [
        "### Sequence Length\n",
        "\n",
        "Neural Networks of any kind operate on matrices of fixed sizes. Part of the definition of a language includes the ability to generate sequences of arbitrary length.\n",
        "\n",
        "#### Solution\n",
        "Depending on your task, there may be several solutions. One option involves padding your sequences to be all the same length. A deep learning model will \"learn\" to ignore the filler words during training. If sequences are very long (generally >500), the \"vanishing gradient\" problem appears where numbers get too small for a computer to use, and nothing can be learned.\n",
        "\n",
        "If you have long sequences, consider breaking the sequences down into shorter sequences. Abstracts were broken down into sentences for this task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpYAAAHKxqMt",
        "colab_type": "text"
      },
      "source": [
        "### Vocabulary Size\n",
        "There are an infinite number of possible words, but we do not have infinite memory or computational power to store and learn them. Additionally, words not seen at training time will appear in future data, either in production systems or during testing.\n",
        "\n",
        "#### Solution\n",
        "\n",
        "Remove rare and unknown words from the data. Words can be removed from sequences or they can be replaced with an \"OOV\" or some other representation of an \"unknown\" value. If replacement is used, the model will learn information about when/where these words appear and may still be able to make correct predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qpg7AZctx0lK",
        "colab_type": "text"
      },
      "source": [
        "### Words are not Numbers\n",
        "The mathematical operations inside a network cannot be performed on words. Sending in the bytes does not make sense for words in the same way it does for other tasks where data might exist in a continuous spectrum, such as the color of a pixel.\n",
        "\n",
        "#### Solution\n",
        "\n",
        "There are many ways to encode text data. The most common for deep learning is with word embeddings, which are a mathematical representation of a word that has been learned by some deep learning model. These can be learned while training your task or they can be taken from other models by downloading them off the internet. Several common embedding sets exist, such as [gloVe](https://nlp.stanford.edu/projects/glove/). To use embeddings, turn each word into a single numeric value that will reference a row in your embedding list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVQ4tl9enE4P",
        "colab_type": "text"
      },
      "source": [
        "## Define Preprocessing Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFpOiRzXm0Rg",
        "colab_type": "text"
      },
      "source": [
        "### Training Data Preprocessing\n",
        "\n",
        "Let's define a method that handles the \"vocabulary size\" and \"words are not numbers\" problem.\n",
        "\n",
        "This method makes indexes for each word and transforms the document into a list of the ids. It also removes \"rare\" words from the vocabulary. In this case a \"rare\" word is defined as something that does not occur more than once in the training set.\n",
        "\n",
        "After processing, instead of sending in `the dog chased the cat`, a network might see `[1,2,3,1,4]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eISL4PIxk8Y2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz84IYs7mg88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_idx_train(document_list):\n",
        "\n",
        "  # get a list of all words and their frequency that appear in the training set\n",
        "  vocab = Counter()\n",
        "  for doc in document_list:\n",
        "    vocab += Counter(doc)\n",
        "  \n",
        "  # check the preprocessed vocab size\n",
        "  print('    Number of tokens in data (unfiltered): {0}'.format(\n",
        "      len(vocab.keys())))\n",
        "\n",
        "  # remove all words that only occur once\n",
        "  vocab_reduced = vocab\n",
        "  for (word,count) in list(vocab_reduced.items()):\n",
        "    if count < 2:\n",
        "      del vocab_reduced[word]\n",
        "  \n",
        "  # check the reduced vocab size\n",
        "  print('    Number of tokens in data (filtered): {0}'.format(\n",
        "      len(vocab_reduced.keys())))\n",
        "  \n",
        "  vocab_final = set(vocab_reduced.keys())\n",
        "  \n",
        "  # build word indexes\n",
        "  word_to_idx = {u:i for i, u in enumerate(vocab_final)}\n",
        "  idx_to_word = list(vocab_final)\n",
        "  \n",
        "  # add PAD word to index manually\n",
        "  word_to_idx['PAD'] = len(idx_to_word)\n",
        "  idx_to_word.append('PAD')\n",
        "\n",
        "  # add OOV word to index manually\n",
        "  word_to_idx['OOV'] = len(idx_to_word)\n",
        "  idx_to_word.append('OOV')\n",
        "  \n",
        "  # replace document with IDs\n",
        "  id_documents =[]\n",
        "  for doc in document_list:\n",
        "    id_doc = []\n",
        "    for word in doc:\n",
        "      # if the word is in our vocab, get the ID\n",
        "      if word in vocab_final:\n",
        "        id_doc.append(word_to_idx[word])\n",
        "      # otherwise get the OOV ID\n",
        "      else:\n",
        "        id_doc.append(word_to_idx[\"OOV\"])\n",
        "    id_documents.append(id_doc)\n",
        "  \n",
        "  return word_to_idx,idx_to_word,id_documents,vocab_final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4in2sddknZuy",
        "colab_type": "text"
      },
      "source": [
        "### Test Data Preprocessing\n",
        "\n",
        "For the test set, remove all words not found in the training vocabulary as unknown. Additionally, removing words that only occur once in the test set it not necessary, because it is possible that the word was learned in the training set already.\n",
        "\n",
        "Replace all words not in the training vocab with \"OOV\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j097lnvnXZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_idx_test(document_list,word_to_idx,vocab):\n",
        "  id_documents = []\n",
        "  \n",
        "  # process document using known words/indexes\n",
        "  for doc in document_list:\n",
        "    id_doc = []\n",
        "    for word in doc:\n",
        "      if word in vocab:\n",
        "        id_doc.append(word_to_idx[word])\n",
        "      else:\n",
        "        id_doc.append(word_to_idx['OOV'])\n",
        "    id_documents.append(id_doc)\n",
        "      \n",
        "  return id_documents  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5lbdI8O5Boo",
        "colab_type": "text"
      },
      "source": [
        "### Label Transformation\n",
        "\n",
        "We also need to transofm our labels into IDs as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krh081p043Dr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_idx_label(train_label_list,test_label_list, label_set):\n",
        "  idx_to_label = ['O']\n",
        "\n",
        "  for label in label_set:\n",
        "    idx_to_label.append(\"B-\"+label)\n",
        "    idx_to_label.append(\"I-\"+label)\n",
        "    idx_to_label.append(\"L-\"+label)\n",
        "    idx_to_label.append(\"U-\"+label)\n",
        "\n",
        "  label_to_idx = {u:i for i, u in enumerate(idx_to_label)}\n",
        "\n",
        "  train_id_labels = [[label_to_idx[label] for label in doc] for doc in train_label_list]\n",
        "  test_id_labels = [[label_to_idx[label] for label in doc] for doc in test_label_list]\n",
        "\n",
        "  return idx_to_label,label_to_idx,train_id_labels,test_id_labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqcGQlZnTCe1",
        "colab_type": "text"
      },
      "source": [
        "### Data Padding\n",
        "\n",
        "To solve the sequence length problem, a simple solution is to pad the ends of shorter sequences until they are same length as longer ones. We already added the word \"PAD\" to our vocabulary, so we simply need to fill in space using it. The network will learn to ignore this padding and focus on the real text on its own."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P58TTCaTDHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_seqs(doc_list,label_list, pad_id, o_id, max_len):\n",
        "  # pad the text with the id for PAD\n",
        "  for doc in doc_list:\n",
        "    if len(doc) < max_len:\n",
        "      doc += [pad_id for i in range(max_len-len(doc))]\n",
        "\n",
        "  # pad the labels with the id for O\n",
        "  for doc in label_list:\n",
        "    if len(doc) < max_len:\n",
        "      doc += [o_id for i in range(max_len-len(doc))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vMaRMNzi4le",
        "colab_type": "text"
      },
      "source": [
        "## Perform Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK8CCBVKLkVR",
        "colab_type": "text"
      },
      "source": [
        "Let's split the data into a sequence of words and labels rather than a string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6UtaFZvTbS8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = [row[1].split() for row in train_data_raw]\n",
        "train_labels = [row[1].split() for row in train_labels_raw]\n",
        "\n",
        "test_data = [row[1].split() for row in test_data_raw]\n",
        "test_labels = [row[1].split() for row in test_labels_raw]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1f2kUOajDxP",
        "colab_type": "text"
      },
      "source": [
        "### Transform into IDs\n",
        "\n",
        "Let's convert the data into the preprocessed format. The data is sorted ahead of time and the row index of the texts and labels can be used in place of the IDs for now. We will also break up the labels and words into a list instead of a string.\n",
        "\n",
        "This step would normally consist of tokenizing documents and aligning labels to your text, but since this is a step not unique to deep learning, it has been done ahead of time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDjMCWDTjY8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_idx,idx_to_word,train_idx,vocab = get_idx_train(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbr3hnilLwu3",
        "colab_type": "text"
      },
      "source": [
        "To check that this method worked... look up a word in the word to index dictionary and see what index it outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8SFztLpsNru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id = word_to_idx['viral']\n",
        "print(id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X-jLL3CL4_z",
        "colab_type": "text"
      },
      "source": [
        "Then take that index and put it in the index to word dictionary and ensure that they reference each other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6-1RsJXsR3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx_to_word[id]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0azFVbZKMGjN",
        "colab_type": "text"
      },
      "source": [
        "Then preprocess the testing data and the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF6nF_PYsryu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_idx = get_idx_test(test_data,word_to_idx,vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXVstRp_B3WG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx_to_label,label_to_idx,train_labels_idx,test_labels_idx = get_idx_label(\n",
        "    train_labels,test_labels,[\"ANATOMY\",\"PHARM\",\"DISEASE\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLI8z8ebTUyp",
        "colab_type": "text"
      },
      "source": [
        "### Pad Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9Z2aUbrMKb6",
        "colab_type": "text"
      },
      "source": [
        "Now we need to pad our data to make sure all of our sequences are the same length. We can start by finding the maximum length of a sequence in our training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veWQcbajThiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length = 0\n",
        "for doc in train_idx:\n",
        "  if len(doc) > max_length:\n",
        "    max_length = len(doc)\n",
        "\n",
        "print(max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HnDONFz2Ogn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_length = max_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4NKSnmoMUiT",
        "colab_type": "text"
      },
      "source": [
        "Then padding all of our sequences to that length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY3abUnXT-z5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pad_seqs(train_idx,train_labels_idx,word_to_idx['PAD'],label_to_idx['O'],seq_length)\n",
        "pad_seqs(test_idx,test_labels_idx,word_to_idx['PAD'],label_to_idx['O'],seq_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IJ6KzWFjMDo",
        "colab_type": "text"
      },
      "source": [
        "# Building a Sequence Tagger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zN1dHkYMZxD",
        "colab_type": "text"
      },
      "source": [
        "To build our deep learning model, we are using Tensorflow and Keras, two of the most common deep learning libraries in python. The most recent version of Tensorflow directly incorporates Keras and simplifies using both.\n",
        "\n",
        "These libraries have extensive, well documented features beyond what is shown here. Full documentation is available on the [tensorflow website](https://www.tensorflow.org/api_docs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDup9aBoHGzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "seed = 111 # deep learning has a random start to most algorithms, this\n",
        "           # seed ensures that everything stays the same between runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCWD0bMK2Jwk",
        "colab_type": "text"
      },
      "source": [
        "To start, we want to define our model. We want a sequential model for this task. Sequential in this case does not mean inputting sequences, it means that each layer in the model is sequential. Layer A provides the input for layer B, layer B provides input for layer C and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaVUMZzc2FXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIXTVOyJNfXW",
        "colab_type": "text"
      },
      "source": [
        "The first layer is an input layer. This simply defines the shape of the data the model will take in. In our case we will take some number of sequences of `seq_length`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrHzj2JtPoPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(keras.layers.Input(shape=(seq_length,)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn8TAF1XN_Ik",
        "colab_type": "text"
      },
      "source": [
        "The next layer is the embedding layer. We already transformed our data to handle this and we will be learning the embeddings at training time. The only parameters we need to give are the `input_dim`, which is the size of our vocabulary (how many embeddings the model needs to make), and the size of the embeddings. Imagine a matrix, we defined how many rows as one per word in our vocabulary, `output_dim` is how many columns are associated with each row."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjiltcpH2QCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(keras.layers.Embedding(input_dim=len(idx_to_word),output_dim=100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMwkmvs8z0ef",
        "colab_type": "text"
      },
      "source": [
        "Dropout is the next layer type added. This means information learned in the embedding layer is randomly thrown away or \"dropped out\" of the model. The theory behind this is that it forces other information to compensate, resulting in stronger learning. This usually helps prevent the model from memorizing the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkGtI40mueah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(keras.layers.Dropout(0.2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM3lKA74zy8O",
        "colab_type": "text"
      },
      "source": [
        "The next layer is the primary layer that learns our task. We want to use a layer called a \"Bidirectional LSTM\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUCVGImK2s3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(keras.layers.Bidirectional(\n",
        "    keras.layers.LSTM(units=50,return_sequences=True)\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ihvBxm94dZZ",
        "colab_type": "text"
      },
      "source": [
        "Dropout is usually added between all layers other than the input layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdrEyZwouqrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(keras.layers.Dropout(0.2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2PsKE7w4iwG",
        "colab_type": "text"
      },
      "source": [
        "This layer is what allows us to predict results over the entire sequence. It treats a sequence of words as a time series problem, like a weather forecast or stock market data, where a value is produced for every time step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOThiJzv3TTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(keras.layers.TimeDistributed(\n",
        "    keras.layers.Dense(len(idx_to_label),activation='softmax')\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwPit0ZS4zOS",
        "colab_type": "text"
      },
      "source": [
        "Compiling the model after it has been made tells your computer how to use the layers we just added. The `optimizer` explains how to move around the high dimensional space that your data exists in. `loss` explains how to penalize incorrect predictions.\n",
        "\n",
        "There are many [optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers), [losses](https://www.tensorflow.org/api_docs/python/tf/keras/losses), and [metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics) available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3-XMYF84SC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcYjY_IB5OBW",
        "colab_type": "text"
      },
      "source": [
        "Afterwards, we can print out a summary of the model to make sure everything looks right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foPFMwU8E8-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvSng5ermLjG",
        "colab_type": "text"
      },
      "source": [
        "## Training the Sequence Tagger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUFqUv1X5XLI",
        "colab_type": "text"
      },
      "source": [
        "Training involves finding the best \"fit\" of the data to the model we defined above, given the training data, a batch size, and how many iterations of the data to go through. As training progresses, `loss` should trend down and `accuracy` should trend up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW7Z7ByVWT_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(x=train_idx,y=train_labels_idx,batch_size=50,epochs=10,verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcq6loJrm_27",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating the Sequence Tagger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzWzlvr0z-Lq",
        "colab_type": "text"
      },
      "source": [
        "### Measuring Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0vJZny95o_B",
        "colab_type": "text"
      },
      "source": [
        "We can evaluate the model based on the test data and get a simple output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-22I3qdGmq04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.evaluate(x=test_idx,y=test_labels_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtmrEFXS562G",
        "colab_type": "text"
      },
      "source": [
        "The first thing to notice out of these results is exceedingly high accuracy, which should be suspicious for most NLP tasks, especially on complex data like clinical text or these abstracts. Additionally, the loss on the training set is lower than on the test set, indicating the model might be overfit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BILqI4VNZ7De",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = model.predict_classes(x=test_idx,batch_size=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1_--DXp6R4V",
        "colab_type": "text"
      },
      "source": [
        "So let's look at some of the output in the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyYgH7f9qskf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds[24]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flPGzfw_6XRh",
        "colab_type": "text"
      },
      "source": [
        "This particular example has one label that isn't an \"O\" in the entire sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3G1qE53I0CX7",
        "colab_type": "text"
      },
      "source": [
        "### Other Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXp32Xxk6hZs",
        "colab_type": "text"
      },
      "source": [
        "NLP usually uses different metrics than simple accuracy because of the example shown above. A model can be highly accurate if all it does is predict the most common label in the dataset. In our case, \"O\" makes up almost all of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IgEvlPsrmVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Afag-RVn61NE",
        "colab_type": "text"
      },
      "source": [
        "Instead, precision, recall, and f1 are used. These provide an accuracy-like result, but are more informative to on rare labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ujd8dKMPxvuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(classification_report(np.array(test_labels_idx).flatten(),preds.flatten(),target_names=idx_to_label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8pebFkW7D5q",
        "colab_type": "text"
      },
      "source": [
        "In this report, the \"O\" label is very accurate, but makes up 98% of the data. This is a common problem in NLP, where the number of words in a document or sentence drown out the rare, relevant information.\n",
        "\n",
        "Some of the more frequent labels such as \"U-DISEASE\" are accurate, but the \"I-\" labels are not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSEecGjt7uu-",
        "colab_type": "text"
      },
      "source": [
        "# Using the Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf-x3cY8-Yv-",
        "colab_type": "text"
      },
      "source": [
        "Next, let's briefly go over some ways we can use this model we've trained or visualize the output on text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAw7wnkPKjLV",
        "colab_type": "text"
      },
      "source": [
        "## Spacy Component"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJjGknVm-i7H",
        "colab_type": "text"
      },
      "source": [
        "Spacy is a natural language processing library that is as common in NLP as tensorflow/keras are in deep learning. It has the ability to use out-of-the-box models trained on their data, add custom models like we will do below, and supports rule-based NLP development."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzwejTajKm9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "from spacy import displacy\n",
        "from spacy.tokens import Span\n",
        "from spacy.lang.en import English"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e1yySp3RNEb",
        "colab_type": "text"
      },
      "source": [
        "A spacy \"component\" is a class that defines the `__call__` method and does something to a piece of text sent in when that method is used.\n",
        "\n",
        "For this tutorial, it breaks the text into chunks of `seq_length` that we defined earlier and predicts the BILUO tags based on the model we just trained and then attaches them to the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRAesx9JKpNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BiLSTMNER:\n",
        "\n",
        "  def __init__(self,model,idx_to_label,word_to_idx,seq_length):\n",
        "    ''' Initializing the spacy component. We want to pass in our trained\n",
        "    model, our label dictionary, and our word dictionary.\n",
        "    '''\n",
        "    self.model = model\n",
        "    self.idx_to_label = idx_to_label\n",
        "    self.word_to_idx = word_to_idx\n",
        "    self.seq_length = seq_length\n",
        "\n",
        "  def __call__(self,doc):\n",
        "    ''' This is the method called when you run: doc = nlp(\"Your text here\")\n",
        "\n",
        "    We want to predict the labels for documents by converting the spacy doc\n",
        "    into an object usable by Tensorflow, i.e. a list of IDs instead of tokens.\n",
        "\n",
        "    Then, we want to save the predictions from Tensorflow into the doc.\n",
        "    '''\n",
        "\n",
        "    subsequence = []\n",
        "\n",
        "    # break document into seq_length chunks and predict\n",
        "    for token in doc:\n",
        "      if token.lower_ in self.word_to_idx.keys():\n",
        "        subsequence.append(self.word_to_idx[token.lower_])\n",
        "      else:\n",
        "        subsequence.append(self.word_to_idx['OOV'])\n",
        "      if len(subsequence) == self.seq_length:\n",
        "        self.make_preds(doc,subsequence)\n",
        "        subsequence = []\n",
        "    \n",
        "    # if there are leftover words, pad the data\n",
        "    if len(subsequence) > 0:\n",
        "      subsequence += [self.word_to_idx['PAD'] \n",
        "                      for i in range(self.seq_length-len(subsequence))]\n",
        "      self.make_preds(doc,subsequence)\n",
        "\n",
        "    return doc\n",
        "\n",
        "  def make_preds(self,doc,sequence):\n",
        "    '''This is a helper method to predict BILUO labels and attach them to \n",
        "    documents.\n",
        "\n",
        "    Although BILUO labels have a specific expected structure, for the purposes\n",
        "    of having more entities shown in visualizations, ANY consecutive sequence of\n",
        "    BIL tags with the same label are considered an entity.\n",
        "\n",
        "    U and O tags are treated as expected.\n",
        "    '''\n",
        "    preds = self.model.predict_classes([sequence])[0]\n",
        "    preds_biluo = [self.idx_to_label[pred] for pred in preds]\n",
        "\n",
        "    entities = []\n",
        "    start = None\n",
        "    label = None\n",
        "    for i, tag in enumerate(preds_biluo):\n",
        "      if tag.startswith(\"U\"):\n",
        "        doc.ents = list(doc.ents) + [Span(doc,i,i,tag[2:])]\n",
        "      if start is not None:\n",
        "        if tag == \"O\" or tag.startswith(\"U\"):\n",
        "          doc.ents = list(doc.ents) + [Span(doc,start,i-1,label)]\n",
        "          start = None\n",
        "        elif tag[2:] != label:\n",
        "          doc.ents = list(doc.ents) + [Span(doc,start,i-1,label)]\n",
        "          start = i\n",
        "      else:\n",
        "        if tag != \"O\":\n",
        "          start = i\n",
        "          label = tag[2:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qla7STpq_dvh",
        "colab_type": "text"
      },
      "source": [
        "To start using spacy, we need to start with an English model, since our text is (mostly) English."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmVzm1d5RhWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = English()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4Q1FRnE_mAq",
        "colab_type": "text"
      },
      "source": [
        "Then add the ability to detect words and sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf3aeQFI_sCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp.add_pipe(nlp.create_pipe('sentencizer'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4H75uJs_0Ai",
        "colab_type": "text"
      },
      "source": [
        "And finally add our custom component that will predict the entites."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfNWf-n7aqms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bilstm = BiLSTMNER(model,idx_to_label,word_to_idx,seq_length)\n",
        "\n",
        "nlp.add_pipe(bilstm, name='bilstm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM4B4MIL74_2",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing Output on New Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqxCbso3_7pZ",
        "colab_type": "text"
      },
      "source": [
        "Spacy was covered quickly, but the purpose of using spacy is to be able to use a library within spacy called displacy. It is a visualization tool for our prediction results. This makes the results much more human readable than a sequence of IDs or BILUO tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRaJsa_y8KpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp(validation_data[0][1])\n",
        "spacy.displacy.render(doc,style='ent',jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H_3Y4yUnqI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp(validation_data[1903][1])\n",
        "spacy.displacy.render(doc,style='ent',jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcPQpH_DMT-K",
        "colab_type": "text"
      },
      "source": [
        "We notice even more now that the >95% accuracy is misleading. Sequences often do not have all the data labeled and if the data is labeled, it might label only part of the true correct phrase.\n",
        "\n",
        "There are two reasons why this could be the case:\n",
        "\n",
        "1.   The model did not learn what was in the data, due to the shape/type of the model, training time, or other factors.\n",
        "2.   The data did not exist in the data for the model to learn from.\n",
        "\n",
        "Pinpointing which of the two is difficult and solving either one can be equally difficult.\n",
        "\n",
        "If after further error analysis, you find part of the model to be satisfactory but wish to improve another aspect, it is easy (and relatively fast) to blend in post-processing and rule-based NLP into the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWMpLC2etA3R",
        "colab_type": "text"
      },
      "source": [
        "## Adding Rule-Based Components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmyVu7ExL0gI",
        "colab_type": "text"
      },
      "source": [
        "In one of the above examples, we noticed that the phrase \"COVID-19\" was missing from the labels. Considering this is a COVID-19 related dataset, that is a large oversight.\n",
        "\n",
        "Luckily, Spacy allows us to quickly add new entities to documents with a few rules from their built in \"EntityRuler\" Feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KqqEQD88d5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy.pipeline import EntityRuler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkZArTzGMI8y",
        "colab_type": "text"
      },
      "source": [
        "We create a new EntityRuler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jbd6v0t8jQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ruler = EntityRuler(nlp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0drxm4qMMDC",
        "colab_type": "text"
      },
      "source": [
        "Add some rules in a JSON format. If there is a match on `pattern`, `label` will be added to the document. We can even add entirely new labels, such as symptoms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIWaBFG28m_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rules = [{\"label\": \"DISEASE\", \"pattern\": [{\"LOWER\": \"covid-19\"}]},\n",
        "         {\"label\": \"SYMPTOM\", \"pattern\": [{\"LOWER\": \"cough\"}]},\n",
        "         {\"label\": \"SYMPTOM\", \"pattern\": [{\"LOWER\": \"dyspnea\"}]},\n",
        "         {\"label\": \"SYMPTOM\", \"pattern\": [{\"LOWER\": \"abdominal\"},{\"LOWER\":\"pain\"}]}]\n",
        "\n",
        "ruler.add_patterns(rules)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyDLVG_EtGHi",
        "colab_type": "text"
      },
      "source": [
        "Add the rules to our nlp system after our trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-OquqJO9mTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp.add_pipe(ruler,name='entity_ruler')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcjHJU1OMm7y",
        "colab_type": "text"
      },
      "source": [
        "And visualize the sentence again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm2z2ldd9tLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp(validation_data[1903][1])\n",
        "spacy.displacy.render(doc,style='ent',jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4mhQt4NMtfE",
        "colab_type": "text"
      },
      "source": [
        "#### Other Spacy Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCseoyjgMx0y",
        "colab_type": "text"
      },
      "source": [
        "For clinical text, further information is often needed, such as the context surrounding any one entity.\n",
        "\n",
        "These can be added into our system as well.\n",
        "\n",
        "Medspacy is one library in development by clinical NLP researchers at the VA and University of Utah that augments spacy's effectiveness on clinical text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVWrY1tvj2L4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install medspacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8LaEJJ9NQvH",
        "colab_type": "text"
      },
      "source": [
        "We will quickly look at the context around our entities in this sentence and visualize them again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct6fXnlPtHFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from medspacy.context import ConTextComponent\n",
        "from medspacy.visualization import visualize_dep"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nH-m8KSNVOd",
        "colab_type": "text"
      },
      "source": [
        "Make the spacy component with default configurations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9eWy9lokikt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context = ConTextComponent(nlp)\n",
        "\n",
        "nlp.add_pipe(context,name='context')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPejx0_ZNexA",
        "colab_type": "text"
      },
      "source": [
        "Visualize the result again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3jJMrVpldNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp(validation_data[1903][1])\n",
        "visualize_dep(doc,jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Wq-BMbN75KD",
        "colab_type": "text"
      },
      "source": [
        "# What Next?\n",
        "\n",
        "The system shown in this tutorial was simple and not very accurate due to complex, sparse data. What else can be done?\n",
        "\n",
        "Unfortunately, iterative development and machine learning generally do not mix. It is hard to guarantee any action improves the quality of the model. However, there are some common next steps that you can try:\n",
        "\n",
        "1.   Tune the model to your data. This example was only run on one specific set of dimensions and epochs. Try a different loss function, smaller embeddings, a bigger LSTM size, more dropout, less dropout, etc.\n",
        "2.   Change the shape of the model. Instead of LSTM cells, try GRU. Instead of an RNN, try a CNN or Transformer. Try adding a CRF. Try two or more layers of LSTMs.\n",
        "3.   Try pre-trained word embeddings or contextual embeddings.\n",
        "4.   Try a traditional machine learning (not deep learning) approach.\n",
        "5.   Augment the system with rules or other pre and post-processing steps.\n",
        "6.   Change project scope or concept definitions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c5g3wX90uj7",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "This tutorial focuses on some of the most common practical challenges posed by deep learning for NLP tasks.\n",
        "\n",
        "Much of what is shown here is now built into the libraries used, however it is often unclear what each method is doing and the reason for doing it in example code and documentation.\n",
        "\n",
        "This model shown was simple. Deep learning models can be made as complicated as you can imagine (and pay for the computers to train on). Current state of the art systems are an order of magnitude larger and often take weeks or months to train on computers far larger than those available at the VA or University of Utah's Center for High Performance Computing. However, that does not mean that bigger models are always better. Smaller models are faster, cheaper, and often still perform better on your specific task. Determining what will work for any one task is far more of an art than a science and will never be fully covered in one tutorial.\n",
        "\n",
        "However, the steps shown surrounding the creation and training of a model are applicable to any NLP deep learning task, not just the one used as an example. That is why improving the performance of this particular model was not the focus.\n",
        "\n",
        "Deep learning and NLP are becoming increasingly mixed and dominating much of the latest research from all angles of NLP (computer science NLP, computational linguistics, clinical NLP, etc.). Understanding how these models are created and what they can and cannot do is a key component of understanding the accomplishments of many researchers in the field, potentially applying it to your own work, and cutting through disinformation and hype surrounding this technology. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MykbSz0OSTbn",
        "colab_type": "text"
      },
      "source": [
        "# Resources\n",
        "\n",
        "This colab notebook will be publicly available indefinitely, as will the [git repository](https://github.com/turbosheep/deep_learning_for_cNLP_AMIA2020) storing the training and testing data.\n",
        "\n",
        "Some additional resources listed below, in no particular order:\n",
        "\n",
        "* Python\n",
        "  * [SpaCy](https://spacy.io/)\n",
        "    * [MedSpaCy](https://github.com/medspacy) (developed and used by VINCI+University of Utah)\n",
        "  * [NLTK](http://www.nltk.org/)\n",
        "  * [CoreNLP](https://stanfordnlp.github.io/CoreNLP/)\n",
        "  * [AllenNLP](https://allennlp.org/)\n",
        "  * [HuggingFace](https://huggingface.co/)\n",
        "* Java\n",
        "  * [OpenNLP](https://opennlp.apache.org/)\n",
        "  * [Stanford NLP](https://nlp.stanford.edu/software/index.shtml)\n",
        "  * [Leo](http://department-of-veterans-affairs.github.io/Leo/) (developed and used by VINCI)\n",
        "\n",
        "Other software packages that may be useful for clinical NLP users and developers:\n",
        "* [CLAMP](https://clamp.uth.edu/)\n",
        "* [cTAKES](https://ctakes.apache.org/)\n",
        "* [Metamap](https://metamap.nlm.nih.gov/)\n",
        "\n",
        "Other tools that are useful for development but are not NLP-specific:\n",
        "* Data Analysis\n",
        "  * [Numpy](https://numpy.org/)\n",
        "  * [Scipy](https://www.scipy.org/)\n",
        "  * [Matplotlib](https://matplotlib.org/)\n",
        "  * [Plotly](https://plotly.com/)\n",
        "* Machine Learning\n",
        "  * [Scikit-Learn](https://scikit-learn.org/stable/)\n",
        "  * [CRF++](https://taku910.github.io/crfpp/)\n",
        "* Deep Learning\n",
        "  * [Tensorflow](https://www.tensorflow.org/)\n",
        "  * [PyTorch](https://pytorch.org/)\n",
        "  * [Keras](https://keras.io/)\n",
        "\n",
        "General Resources:\n",
        "* [StackOverflow](https://stackoverflow.com/), specifically the tensorflow, keras, spacy, etc. tags\n",
        "* [TowardsDataScience](https://towardsdatascience.com/), helpful articles with tutorials and examples of many kinds of deep learning tasks\n",
        "* [MachineLearningMastery](https://machinelearningmastery.com/), another source of help articles and tutorials\n",
        "* [Deep Learning for NLP Best Practices](https://ruder.io/deep-learning-nlp-best-practices/), an article covering some of what was covered here plus a lot of other helpful tricks and good habits\n",
        "* [Neural Network Methods for Natural Language Processing](https://www.morganclaypoolpublishers.com/catalog_Orig/product_info.php?products_id=1056), a textbook written by a prominent NLP researcher and professor covering many deep learning NLP tasks in detail\n",
        "* The Issues page on Github for any of the linked libraries can also be useful for troubleshooting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DRWqqWBRw_K",
        "colab_type": "text"
      },
      "source": [
        "# Questions?\n",
        "\n",
        "If there are further questions about the contents of this demonstration, feel free to reach out to Olga (olga.patterson@utah.edu) or Hannah (hannah.eyre@utah.edu)\n",
        "\n",
        "If you have questions about NLP development in VINCI environments or questions about what NLP-developed datasets are available to VA researchers, please send an email to vinci@va.gov"
      ]
    }
  ]
}